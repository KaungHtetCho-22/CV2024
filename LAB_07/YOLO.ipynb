{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b732b1af-ae18-489f-a6c5-2a19072b7f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st124092/work/cuda116/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfa4a81-2df4-488c-9b21-ae6b5eff0d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cuda' if torch.cuda.is_available() else'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df608bf0-e2cb-46f0-b8f6-17763813b80b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# YOLO ARCHITECTURE\n",
    "\n",
    "<img src=\"./assets/YOLO_Architecture_from_the_original_paper_.png\">\n",
    "\n",
    "Image Reference: https://www.datacamp.com/blog/yolo-object-detection-explained\n",
    "\n",
    "The following code is inspired and taken from this repo : https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16208d60-7377-42a7-b2f8-b14e5a9b30e2",
   "metadata": {},
   "source": [
    "## Lets build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ee6dbe-057e-4ba6-92cb-ed467824dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (kernel_size, filters, stride, padding) \n",
    "\"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "List is structured by tuples and lastly int with number of repeats\n",
    "\"\"\"\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4], # list architecture\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc04377f-3b8d-48b2-9641-ef3b4eaca84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)            # Not used in original implementation!\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4465d76b-d01e-4908-9e6c-6721ea84736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fc(**kwargs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "    \n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(in_channels, out_channels = x[1], kernel_size=x[0], stride=x[2], padding=x[3],)]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            if type(x) == str:\n",
    "                layers += [\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "\n",
    "            if type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(in_channels, out_channels = conv1[1], kernel_size= conv1[0], stride=conv1[2], padding=conv1[3],)]\n",
    "                    layers += [\n",
    "                        CNNBlock(conv1[1], out_channels = conv2[1], kernel_size= conv2[0], stride=conv2[2], padding=conv2[3],)]\n",
    "                    in_channels = conv2[1]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _create_fc(self, split_size, num_boxes, num_classes):\n",
    "        S,B,C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*S*S , 4096),\n",
    "            nn.Dropout(0.5),                         # not implemented in paper\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, S*S*(C + B*5)),           # C+5*B = 30\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a017db-3cba-4572-843e-7b17f0bc099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yolov1(\n",
      "  (darknet): Sequential(\n",
      "    (0): CNNBlock(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): CNNBlock(\n",
      "      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): CNNBlock(\n",
      "      (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (5): CNNBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (6): CNNBlock(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (7): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): CNNBlock(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (10): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (11): CNNBlock(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (12): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (13): CNNBlock(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (14): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (15): CNNBlock(\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (16): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (17): CNNBlock(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (18): CNNBlock(\n",
      "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (20): CNNBlock(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (21): CNNBlock(\n",
      "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (22): CNNBlock(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (23): CNNBlock(\n",
      "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (24): CNNBlock(\n",
      "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (25): CNNBlock(\n",
      "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (26): CNNBlock(\n",
      "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "    (27): CNNBlock(\n",
      "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "    )\n",
      "  )\n",
      "  (fcs): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=50176, out_features=4096, bias=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): Linear(in_features=4096, out_features=1470, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test our model\n",
    "\n",
    "model = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ceaf167-027b-4dc8-b60e-ff690a479b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1470])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 448, 448)\n",
    "print(model(x).shape)                      # 7*7*30 = 1470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a6b7f-b1f9-4197-bec3-581c8103047c",
   "metadata": {},
   "source": [
    "## Lets build our Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8d3861-97c8-4818-8f56-5739e18dbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import VOCDataset\n",
    "from utils import intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d15f7a-bce6-4758-8da5-30c41a0839e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e21f3d-f432-4460-a60b-f47c3a90ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    intersection_over_union,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7af357-de52-4385-96ce-6e42ae0ed390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee807d5610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ced8bc-59fb-44e7-9a4b-d3491b062092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc. \n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"best_so_far.pth.tar\"\n",
    "IMG_DIR = \"dataset/images\"\n",
    "LABEL_DIR = \"dataset/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5639997-ad3a-419d-bf83-972852324415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8781a33-a4f6-4cd4-8dbe-c78d4eb7f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a47ddb42-741e-4944-96ad-ee7ddbd871a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    train_dataset = VOCDataset(\n",
    "        \"dataset/100examples.csv\",\n",
    "        transform=transform,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    test_dataset = VOCDataset(\n",
    "        \"dataset/test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "        #if mean_avg_prec > 0.9:\n",
    "        #    checkpoint = {\n",
    "        #        \"state_dict\": model.state_dict(),\n",
    "        #        \"optimizer\": optimizer.state_dict(),\n",
    "        #    }\n",
    "        #    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "        #    import time\n",
    "        #    time.sleep(10)\n",
    "\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9a01434-5987-4a0d-97e6-2c1954dfe339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.02it/s, loss=715]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 889.3602294921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.56it/s, loss=395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 537.6829884847006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.43it/s, loss=503]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 478.6702575683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.62it/s, loss=526]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 435.08201090494794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.64it/s, loss=306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 368.8504587809245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 6.61375597701408e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.40it/s, loss=281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 321.43519083658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 3.561253106454387e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.28it/s, loss=202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 282.7339324951172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.21it/s, loss=231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 252.1270980834961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 5.733943544328213e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.44it/s, loss=265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 251.42701212565103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.00028062198543921113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.23it/s, loss=214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 229.84151458740234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66417f3f-3a25-4e4a-8ed8-e1535d19a26b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### write a test function for validation data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38fffaa-e295-4d16-9f93-6af47287f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    mean_loss = []\n",
    "    loop = tqdm(valid_loader, leave=True)\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            # update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    print(f\"Mean validation loss was {sum(mean_loss)/len(mean_loss)}\")\n",
    "    model.train()  \n",
    "    return sum(mean_loss)/len(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73401f13-843a-4b20-b1e3-7742ffaa0055",
   "metadata": {},
   "source": [
    "### train the yolo model with training and validation set on PASCAL VOC data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75247d2b-a82b-4983-9432-57ae49155c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 16550\n",
      "Training set size: 13240 (80.0%)\n",
      "Validation set size: 3310 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "full_dataset = VOCDataset(\n",
    "    \"dataset/train.csv\",\n",
    "    transform=transform,\n",
    "    img_dir=IMG_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    ")\n",
    "\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "train_indices, val_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=0.2, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Training set size: {train_size} ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"Validation set size: {val_size} ({val_size/total_size*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca07db6-4616-40d3-b161-7ce0819d69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "        val_loss = valid_fn(val_loader, model, loss_fn)\n",
    "        \n",
    "        checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "    print(\"Model saved after 10 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b579637-c957-4b78-b544-83504bf21128",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c76d238b-a09e-4351-adf8-dd222c374513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:24<00:00,  4.04it/s, loss=192]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 251.90797998135437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:52<00:00,  3.96it/s, loss=163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 163.26945588195207\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.002744792029261589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:02<00:00,  4.53it/s, loss=182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 179.07935164427383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:44<00:00,  4.64it/s, loss=153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 162.85283397933813\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.007169117219746113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [04:43<00:00,  2.92it/s, loss=135] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 161.70346530605255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [01:13<00:00,  2.82it/s, loss=148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 148.96882344218133\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.012085916474461555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [04:05<00:00,  3.37it/s, loss=128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 152.16088289678314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [01:09<00:00,  2.95it/s, loss=146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 151.80737538013642\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.015234148129820824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:06<00:00,  4.43it/s, loss=136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 145.9815778074991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [02:26<00:00,  1.41it/s, loss=139]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 144.69658575706111\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.019742552191019058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [05:48<00:00,  2.38it/s, loss=128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 142.86766745072612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:53<00:00,  3.82it/s, loss=150] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 147.16873750408877\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.015162885189056396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:35<00:00,  3.83it/s, loss=163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 136.3023274063055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:54<00:00,  3.80it/s, loss=152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 137.13396129793333\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.02364708110690117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:16<00:00,  4.20it/s, loss=85.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 130.03348006187355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:45<00:00,  4.55it/s, loss=130] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 122.83234498107318\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.03647088259458542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [03:09<00:00,  4.36it/s, loss=121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 124.56289040912596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:54<00:00,  3.79it/s, loss=138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 121.81533954212966\n",
      "=> Saving checkpoint\n",
      "Train mAP: 0.033590167760849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [02:51<00:00,  4.83it/s, loss=112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was 118.8215225754799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:50<00:00,  4.10it/s, loss=133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation loss was 136.5670263420031\n",
      "=> Saving checkpoint\n",
      "Model saved after 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec489029-ca70-402b-b857-5c0adf0d18f0",
   "metadata": {},
   "source": [
    "###  the performance of trained model on test data of PASCAL VOC in terms of mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9af3a023-7c4f-4743-98ec-dcc4386203ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = True\n",
    "LOAD_MODEL_FILE = \"best_so_far.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a6a456-09f4-41f5-80af-ddc1167cf2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "Model loaded successfully from best_so_far.pth.tar\n"
     ]
    }
   ],
   "source": [
    "model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# if LOAD_MODEL:\n",
    "#     load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "#     print(\"Model loaded successfully from\", LOAD_MODEL_FILE)\n",
    "    \n",
    "if LOAD_MODEL:\n",
    "    checkpoint = torch.load(LOAD_MODEL_FILE)\n",
    "    load_checkpoint(checkpoint, model, optimizer)  # Use the imported function\n",
    "    print(\"Model loaded successfully from\", LOAD_MODEL_FILE)\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    \"dataset/test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4459ff14-d5a4-4bef-9beb-cb6f3b79ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set mAP: 0.0251\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_test_set(test_loader, model, device, iou_threshold=0.5, conf_threshold=0.4):\n",
    "    model.eval() \n",
    "    pred_boxes, target_boxes = [], []\n",
    "\n",
    "    model = model.to(device)  \n",
    "\n",
    "    with torch.no_grad():  \n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            test_loader, model, iou_threshold=iou_threshold, threshold=conf_threshold, device=device\n",
    "        )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=iou_threshold, box_format=\"midpoint\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Set mAP: {mean_avg_prec:.4f}\")\n",
    "\n",
    "evaluate_test_set(test_loader, model, DEVICE, iou_threshold=0.5, conf_threshold=0.4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel-gpu",
   "language": "python",
   "name": "kernel-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
