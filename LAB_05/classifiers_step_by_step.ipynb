{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b947f6b-203a-40e8-ba2c-242bdf3239cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Classifier with Gradient Descent\n",
    "\n",
    "--- \n",
    "\n",
    "In this lab, we will explore the implementation of a linear classifier from scratch. The topics covered include:\n",
    "\n",
    "- Initialization of weights and bias\n",
    "- Matrix multiplication of inputs (X) and weights (theta) with bias\n",
    "- Loss (cost) function calculation\n",
    "- Gradient Descent (both batch and stochastic)\n",
    "- Weight update\n",
    "- Use case for binomial and multinomial classification using sigmoid and softmax\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff5f80-34e3-4535-9dbc-bda8b090df68",
   "metadata": {},
   "source": [
    "## 0. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e51bfee6-a1e8-4177-b3be-dc1838b3c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2100bcd-bc55-4266-8f77-0f9a918a94aa",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Initialization of Weights and Bias\n",
    "\n",
    "Before training, we need to initialize our weights (`theta`) and bias (`b`). This can be done randomly or using a small constant value. In most cases, initializing weights with small random values works best to break symmetry, while bias can be initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b61c6-a70a-4ad4-82be-31e8d9499b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Matrix Multiplication of $X$ and $\\theta$ with Bias\n",
    "\n",
    "In linear models, the prediction is computed as the dot product between the input features $X$ and the weight vector $\\theta$, plus the bias $b$. Mathematically, this is expressed as:\n",
    "\n",
    "$$ \n",
    "y = X\\theta + b \n",
    "$$\n",
    "\n",
    "To incorporate the bias term into the matrix multiplication, we can augment the input matrix $X$ and the weight vector $\\theta$. \n",
    "\n",
    "### i. Augmenting $X$\n",
    "\n",
    "Add a column of ones to the input matrix $X$ to account for the bias term. Let $X$ have dimensions $m \\times n$ (where $m$ is the number of samples and $n$ is the number of features). The augmented matrix $X_{\\text{bias}}$ will have dimensions $m \\times (n+1)$:\n",
    "\n",
    "$$\n",
    "X_{\\text{bias}} = \\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### ii. Augmenting $\\theta$\n",
    "\n",
    "Extend $\\theta$ to include the bias term. The extended vector $\\theta_{\\text{bias}}$ will have dimensions $(n+1) \\times c$ (where c is the no. of classes):\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{bias}} = \\begin{bmatrix}\n",
    "b \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### iii. Matrix Multiplication\n",
    "\n",
    "With the augmented matrix $X_{\\text{bias}}$ and the extended vector $\\theta_{\\text{bias}}$, the prediction $y$ can be computed as:\n",
    "\n",
    "$$\n",
    "y = X_{\\text{bias}} \\theta_{\\text{bias}}\n",
    "$$\n",
    "\n",
    "This approach simplifies the computation by integrating the bias term directly into the matrix multiplication, which can be more efficient and straightforward in practice, especially when using matrix operations libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad6cecb-1838-447d-badd-497a033aeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term (column of 1s) to X\n",
    "def add_bias_term(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9711be-0e40-4b66-a540-2dfbf5113399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights with X updated to handle bias\n",
    "def initialize_parameters(X, y, multiclass):\n",
    "    n_features = X.shape[1]  # Number of features from the input X with bias term\n",
    "    if multiclass:\n",
    "        n_classes = y.shape[1]\n",
    "        theta = np.random.randn(n_features, n_classes) * 0.01  # Small random weights\n",
    "    else:\n",
    "        theta = np.random.randn(n_features, 1) * 0.01  # Small random weights\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03e7e34-8b4f-4ed6-b5b8-7cc59bad86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear prediction\n",
    "def linear_prediction(X, theta):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0f62d-e178-45f8-942e-79aeed198b15",
   "metadata": {},
   "source": [
    "## 3. Loss Functions for Classification\n",
    "\n",
    "For classification tasks, different loss functions are used depending on whether the task is binary classification or multinomial classification.\n",
    "\n",
    "### Binary Classification (Sigmoid/Logistic Regression)\n",
    "\n",
    "The loss function used is binary cross-entropy, also known as log loss. It is defined as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of samples.\n",
    "- $y_i$ is the true label for the $i$-th sample.\n",
    "- $\\hat{y}_i$ is the predicted probability for the $i$-th sample.\n",
    "\n",
    "### Multinomial Classification (Softmax)\n",
    "\n",
    "For multinomial classification, especially after one-hot encoding the labels, the loss function is categorical cross-entropy. It is defined as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of samples.\n",
    "- $K$ is the number of classes.\n",
    "- $y_{ik}$ is the one-hot encoded label for the $i$-th sample and $k$-th class (binary indicator: 0 or 1).\n",
    "- $\\hat{y}_{ik}$ is the predicted probability of class $k$ for the $i$-th sample.\n",
    "\n",
    "In one-hot encoding, $y_{ik}$ is 1 if the $i$-th sample belongs to class $k$, and 0 otherwise. This loss function measures how well the predicted probabilities match the one-hot encoded true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782dd119-6097-480a-bdbd-c259d42119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-15  # To avoid log(0) this is a very small value\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "\n",
    "# Categorical Cross Entropy Loss\n",
    "def categorical_cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4db1e-c7f9-490c-9b08-1bf60298ede9",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent for Minimizing the Loss Function with weight updates\n",
    "\n",
    "Gradient descent is used to minimize the loss function by iteratively updating the weights based on the gradient of the loss function.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "In Batch Gradient Descent, the gradient is computed using all examples in the dataset:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ is the gradient of the loss function with respect to the weights.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In Stochastic Gradient Descent, the gradient is computed using only one example at a time:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ is the gradient of the loss function with respect to the weights, computed for a single example.\n",
    "\n",
    "In both cases, the update rule for weights is the same, but the difference lies in how the gradient is computed: either over the entire dataset (Batch Gradient Descent) or a single example (Stochastic Gradient Descent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabf1c6-8008-404d-81b8-4361c3bccd36",
   "metadata": {},
   "source": [
    "## 5. Weight Update\n",
    "After calculating the gradient, we update the weights using the formula mentioned above. Depending on whether we are using batch gradient descent or stochastic gradient descent, the weight update happens differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2bf0bb2-4b06-4db3-a3f7-2fff679520f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(X, y, predictions, theta, learning_rate, multiclass):\n",
    "    m = X.shape[0]\n",
    "    # predictions = linear_prediction(X, theta)\n",
    "    if multiclass:\n",
    "        # predictions = softmax(predictions)\n",
    "        gradients = (1/m) * np.dot(X.T, (predictions - y))\n",
    "    else:\n",
    "        # predictions = sigmoid(predictions)\n",
    "        gradients = (1/m) * np.dot(X.T, (predictions - y))\n",
    "    theta = theta - learning_rate * gradients\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374cce8d-f2d5-46f0-825c-a12977a4540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD) Step\n",
    "def stochastic_gradient_descent_step(X, y, theta, learning_rate, multiclass):\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        xi = X[i:i+1]\n",
    "        yi = y[i:i+1]\n",
    "        # print (theta.shape)\n",
    "        prediction = linear_prediction(xi, theta)\n",
    "        if multiclass:\n",
    "            prediction = softmax(prediction)\n",
    "        else:\n",
    "            prediction = sigmoid(prediction)\n",
    "        # print(prediction.shape)\n",
    "        # print(xi.T.shape)\n",
    "        # print(yi.shape)\n",
    "        gradients = np.dot(xi.T, (prediction - yi))\n",
    "        theta = theta - learning_rate * gradients\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74618e-f0b1-4f6f-b799-137a5bae23f8",
   "metadata": {},
   "source": [
    "## Activation: Binomial Classification (Sigmoid)\n",
    "\n",
    "In binary classification, the sigmoid function is applied to the linear output to obtain the predicted probability. The sigmoid function $\\sigma(z)$ is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(X \\theta) = \\frac{1}{1 + e^{-X \\theta}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}$ is the predicted probability.\n",
    "- $X \\theta$ represents the linear combination of the input features $X$ and the weights $\\theta$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "\n",
    "The sigmoid function maps the linear output to a probability value between 0 and 1, which can then be used to make a classification decision.\n",
    "\n",
    "## Activation: Multinomial Classification (Softmax)\n",
    "\n",
    "In multinomial classification, the softmax function is used to compute probabilities across multiple classes. The softmax function $\\text{softmax}(z_i)$ for class $k$ is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{ik} = \\frac{e^{(X \\theta_k)}}{\\sum_{j=1}^{K} e^{(X \\theta_j)}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}_{ik}$ is the predicted probability of the $i$-th sample belonging to class $k$.\n",
    "- $X \\theta_k$ is the linear combination of the input features $X$ and the weights $\\theta_k$ for class $k$.\n",
    "- $K$ is the total number of classes.\n",
    "- The denominator is the sum of the exponentials of the linear combinations for all classes, ensuring that the probabilities sum up to 1.\n",
    "\n",
    "The softmax function converts the linear outputs into a probability distribution over multiple classes, which is useful for making predictions in multiclass classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097227b8-878d-4a99-b718-ed4f330282d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function for binary classification\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Softmax function for multi-class\n",
    "def softmax(z):\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a27b9-d93a-4a45-9b9e-f4f5e9223eca",
   "metadata": {},
   "source": [
    "### OKAY LETS GO A HEAD AND TRAIN OUR MODEL !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab915c07-edaf-48af-8b30-205868d47ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(X, y, learning_rate=0.01, iterations=1000, batch=True, multiclass=False):\n",
    "    # Add bias term to X\n",
    "    X = add_bias_term(X)\n",
    "    \n",
    "    # Initialize theta\n",
    "    theta = initialize_parameters(X, y, multiclass)\n",
    "    \n",
    "    # Track loss over iterations\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Compute predictions and loss\n",
    "        if multiclass:\n",
    "            predictions = softmax(linear_prediction(X, theta))\n",
    "            loss = categorical_cross_entropy_loss(y, predictions)\n",
    "        else:\n",
    "            predictions = sigmoid(linear_prediction(X, theta))\n",
    "            loss = binary_cross_entropy_loss(y, predictions)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Update weights\n",
    "        if batch:\n",
    "            theta = gradient_descent_step(X, y, predictions, theta, learning_rate, multiclass)\n",
    "        else:\n",
    "            # Use stochastic gradient descent\n",
    "            theta = stochastic_gradient_descent_step(X, y, theta, learning_rate, multiclass)\n",
    "        \n",
    "        # Print loss every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}/{iterations}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3572f-4f20-496e-a043-1e3db8056911",
   "metadata": {},
   "source": [
    "### Lets generate some data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c81a756-51e8-451d-a513-4c43d7bfbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set multiclass to True or False\n",
    "multiclass = True  # Set to True for multiclass classification\n",
    "\n",
    "if multiclass:\n",
    "    # For multiclass classification\n",
    "    X_syn, y_syn = make_classification(n_samples=200, n_features=3, n_informative=3, \n",
    "                                   n_redundant=0, n_clusters_per_class=1, n_classes=4, random_state=100)\n",
    "    # Convert y to one-hot encoding\n",
    "    y_syn = np.eye(np.max(y_syn) + 1)[y_syn]\n",
    "else:\n",
    "    # For binary classification\n",
    "    X_syn, y_syn = make_classification(n_samples=200, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=100)\n",
    "    y_syn = y_syn.reshape(-1, 1)  # Reshape y to be a column vector\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(X_syn, y_syn, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09637a4b-7c1a-42ad-b141-ac91f65ae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiclass:\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # Convert one-hot encoding to class labels for plotting\n",
    "    y_train_labels = np.argmax(y_train_syn, axis=1)\n",
    "\n",
    "    # Use the 3 features for the scatter plot\n",
    "    scatter = ax.scatter(X_train_syn[:, 0], X_train_syn[:, 1], X_train_syn[:, 2], \n",
    "                         c=y_train_labels, cmap='coolwarm', edgecolor='k', s=100)\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_title(\"3D Scatter Plot of Synthetic Data\")\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_zlabel(\"Feature 3\")\n",
    "\n",
    "    # Add color bar to represent class labels\n",
    "    cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
    "    cbar.set_label('Class')\n",
    "    \n",
    "    # Set ticks to be integers corresponding to class labels\n",
    "    cbar.set_ticks(np.arange(np.min(y_train_labels), np.max(y_train_labels) + 1))\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_train_syn[:, 0], X_train_syn[:, 1], c=y_train_syn, cmap='coolwarm', edgecolor='k', s=100)\n",
    "    plt.title(\"Scatter Plot of Training Data\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2a9e4-db27-49e2-8de5-3f1626571c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "theta, losses = train_model(X_train_syn, y_train_syn, learning_rate=0.01, iterations=1000, batch=True, multiclass=multiclass)\n",
    "print(f\"Time Taken Using Batch gradient descent :{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fca25-c582-4866-9d47-c0e842dc0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab42614-2e40-43b2-94e9-38b3a794c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term to test data\n",
    "X_test_bias = add_bias_term(X_test_syn)\n",
    "\n",
    "if multiclass:\n",
    "    predictions = softmax(linear_prediction(X_test_bias, theta))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test_syn, axis=1)\n",
    "else:\n",
    "    predictions = sigmoid(linear_prediction(X_test_bias, theta))\n",
    "    predicted_classes = (predictions >= 0.5).astype(int)\n",
    "    true_classes = y_test_syn\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes.flatten() == true_classes.flatten())\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61819d-8ac3-441b-93cd-ce716a20a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "theta, losses = train_model(X_train_syn, y_train_syn, learning_rate=0.01, iterations=1000, batch=False, multiclass=multiclass)\n",
    "print(f\"Time Taken Using Stochastic gradient descent :{time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43d74f-9ecf-4202-9c5e-1c22c9a6f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33657d6e-02c3-4d74-8751-3717353dafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term to test data\n",
    "X_test_bias = add_bias_term(X_test_syn)\n",
    "\n",
    "if multiclass:\n",
    "    predictions = softmax(linear_prediction(X_test_bias, theta))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test_syn, axis=1)\n",
    "else:\n",
    "    predictions = sigmoid(linear_prediction(X_test_bias, theta))\n",
    "    predicted_classes = (predictions >= 0.5).astype(int)\n",
    "    true_classes = y_test_syn\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes.flatten() == true_classes.flatten())\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94149a44-925e-4a5d-be31-7db9a1b24f5f",
   "metadata": {},
   "source": [
    "## For Image what could be possible changes??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5b8d50-f27f-431d-896b-ad00312ab5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our puffer surver we need to browse via a proxy!!\n",
    "\n",
    "# Set HTTP and HTTPS proxy\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b3d50-1987-40ed-b7a7-9a7d538f7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db28b1-5e0e-4887-b337-0a0fe69a86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train = datasets.CIFAR10('../LAB04/data', train=True, download=True ,transform=transforms.ToTensor())\n",
    "cifar_test = datasets.CIFAR10('../LAB04/data', train=False, download=True ,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf767063-1870-48d0-95a2-84856cc3e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to subsample CIFAR-10 dataset\n",
    "def subsample_dataset(dataset, sample_size=1000):\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    subset = Subset(dataset, indices)\n",
    "    return subset\n",
    "\n",
    "# Subsample the training and test datasets\n",
    "sample_size = 1000\n",
    "train_subset = subsample_dataset(cifar_train, sample_size=sample_size)\n",
    "test_subset = subsample_dataset(cifar_test, sample_size=int(sample_size * 0.4))\n",
    "\n",
    "# Load data into PyTorch DataLoader\n",
    "train_loader = DataLoader(train_subset, batch_size=sample_size, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=int(sample_size * 0.4), shuffle=False)\n",
    "\n",
    "# Fetch all data and labels for easier handling\n",
    "X_train, y_train = next(iter(train_loader))\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "\n",
    "print(\"Before Flattening\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Reshape the images to 2D for the KNN algorithm\n",
    "X_train = X_train.view(X_train.size(0), -1).to(device)  # Flatten\n",
    "X_test = X_test.view(X_test.size(0), -1).to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "print(\"After Flattening\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc8c92df-6536-42f6-a947-d6f4313f363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLinearClassifier:\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        self.W = np.random.randn(n_classes, input_size) * 0.01  # Small random weights (10,3072)\n",
    "        self.b = np.zeros((n_classes, 1))  # Bias initialized to zero (10,1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Reshape X to (input_size, batch_size)\n",
    "        X=X.T  # Transpose to shape (3072,1000)\n",
    "        return np.dot(self.W, X) + self.b\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "            X: (batch_size, input_size) = (1000, 3072)\n",
    "            y: (batch_size,) = (1000,) with class labels (0-9)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        z = self.predict(X)\n",
    "        probs = self.softmax(z)\n",
    "        log_likelihood = -np.log(probs[y, range(m)])\n",
    "        return np.sum(log_likelihood) / m\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def gradient_descent(self, X, y, learning_rate=0.001):\n",
    "        # Compute the gradient and update W, b\n",
    "        m = X.shape[0]\n",
    "        z = self.predict(X)\n",
    "        probs = self.softmax(z)\n",
    "        probs[y, range(m)] -= 1  # Gradient of softmax loss wrt z\n",
    "        dW = np.dot(probs, X) / m  #Gradient wrt weights\n",
    "        db = np.sum(probs, axis=1, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and bias\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f630178-d8f7-4b34-b4f4-c67e3d77d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, X_train, y_train, epochs, learning_rate):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        loss = classifier.compute_loss(X_train, y_train)\n",
    "        losses.append(loss)\n",
    "        print(f'Epoch {i+1}, Loss: {loss}')\n",
    "        classifier.gradient_descent(X_train, y_train, learning_rate)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf60efc-5499-44b3-b552-3cd6354697d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data: {len(cifar_train)}\")\n",
    "print(f\"Test data: {len(cifar_test)}\")\n",
    "\n",
    "image, label = cifar_train[0]\n",
    "# Now you can check the shape of the image\n",
    "print(f\"Image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13cf78-50d5-4f32-b2fa-359bf19a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "n_classes = 10  # For CIFAR-10\n",
    "image_size = 32 * 32 * 3  # CIFAR-10 images are 32x32x3\n",
    "classifier = ImageLinearClassifier(input_size=image_size, n_classes=n_classes)\n",
    "\n",
    "# X_train is shape (image_size, batch_size) and y_train is (batch_size,)\n",
    "losses = train(classifier, X_train, y_train, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593dde9-814a-4ea8-a6b5-6ceadd97ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e68a42-8276-4a83-9478-34204beec987",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "1. Input Layer: The input data, similar to your previous setup.\n",
    "2. Hidden Layer(s): These layers will have weights, biases, and non-linear activations like ReLU.\n",
    "3. Output Layer: This will have a softmax activation for classification.\n",
    "4. Loss Function: Cross-entropy loss for classification.\n",
    "5. Backpropagation: To update weights using gradients from the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a76926-54c7-4df7-b0d2-587e5ab8044d",
   "metadata": {},
   "source": [
    "## MLP Structure Example:\n",
    "1. Input Layer: (3072 neurons, corresponding to image size 32x32x3 in CIFAR-10)\n",
    "2. Hidden Layer 1: Fully connected, with a non-linear activation like ReLU.\n",
    "3. Hidden Layer 2: Another fully connected layer (optional).\n",
    "4. Output Layer: A fully connected layer with 10 neurons (for 10 classes) and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d4ed4ce-2827-4274-8648-208443c86158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Weight initialization\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01  # (hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))  # (hidden_size, 1)\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01  # (output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))  # (output_size, 1)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        X: input data of shape (input_size, batch_size)\n",
    "        \"\"\"\n",
    "        # Layer 1 (hidden layer)\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1  # (hidden_size, batch_size)\n",
    "        self.A1 = self.relu(self.Z1)  # Apply ReLU activation\n",
    "        \n",
    "        ## ADD ANOTHER HIDDEN LAYER IN YOUR TAKE HOME EXERCISE\n",
    "\n",
    "        # Layer 2 (output layer)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2  # (output_size, batch_size)\n",
    "        self.A2 = self.softmax(self.Z2)  # Apply softmax activation\n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, A2, y):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        A2: output from softmax, shape (output_size, batch_size)\n",
    "        y: true labels, shape (batch_size,)\n",
    "        \"\"\"\n",
    "        m = y.shape[0]  # batch size\n",
    "        log_likelihood = -np.log(A2[y, range(m)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Perform backward propagation and update weights.\n",
    "        X: input data of shape (input_size, batch_size)\n",
    "        y: true labels of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        m = X.shape[1]  # Batch size\n",
    "        \n",
    "        # Gradient of the loss w.r.t. Z2\n",
    "        dZ2 = self.A2  # Softmax probabilities\n",
    "        dZ2[y, range(m)] -= 1  # Subtract 1 from the correct class probabilities\n",
    "        dZ2 /= m\n",
    "        \n",
    "        # Gradients for W2 and b2\n",
    "        dW2 = np.dot(dZ2, self.A1.T)  # (output_size, hidden_size)\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)  # (output_size, 1)\n",
    "        \n",
    "        # Gradients for the hidden layer (backprop through ReLU)\n",
    "        dA1 = np.dot(self.W2.T, dZ2)  # (hidden_size, batch_size)\n",
    "        dZ1 = dA1 * self.relu_derivative(self.Z1)  # Backprop through ReLU\n",
    "        \n",
    "        # Gradients for W1 and b1\n",
    "        dW1 = np.dot(dZ1, X.T)  # (hidden_size, input_size)\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)  # (hidden_size, 1)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        X_train: input data, shape (input_size, batch_size)\n",
    "        y_train: true labels, shape (batch_size,)\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            A2 = self.forward(X_train)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = self.compute_loss(A2, y_train)\n",
    "            print(f'Epoch {i+1}, Loss: {loss}')\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X_train, y_train, learning_rate)\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c26ce-e6ae-4de3-9e46-a953f63013c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3072  # CIFAR-10 images are 32x32x3\n",
    "hidden_size = 100  # Arbitrary hidden layer size\n",
    "output_size = 10  # 10 classes for CIFAR-10\n",
    "\n",
    "mlp = MLPClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "X_train_copy = X_train.T\n",
    "\n",
    "losses = mlp.train(X_train_copy, y_train, epochs=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021f300-824c-4e38-97ba-e3964aaed0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6770a-1833-4a3e-abe8-168b2b17e360",
   "metadata": {},
   "source": [
    "## TAKE AWAY EXERCISE\n",
    "\n",
    "- In the picture above the learning rate is high so the model is not able to find a minima and is overshooting.\n",
    "- Use a learning rate scheduler in the above implementation, increase the no. of epochs and train on full data.\n",
    "- Also increase the complexity of structure by adding another hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d09937-193c-47c8-9c66-3c63c6d17183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
